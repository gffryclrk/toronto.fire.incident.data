---
title: "CMTH642 Capstone"
author: "Geoffrey Clark"
date: "May 28, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Fire Incident Data Overview
### Input Data: Incidents
```{r}
## Incidents
# Load & Combine Dataset from Yearwise .csv

I <- read.csv("../csv/2011_i.csv", header=T, stringsAsFactors = F, na.strings=c("","NA", " "))
# importSchema = c("character", "factor", "POSIXct", "POSIXct", "factor", "factor", "factor", "factor", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "factor", "factor", "character", "character", "character", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "character", "factor", "factor", "character", "character", "character", "character", "character", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "character", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "character", "factor", "factor", "factor", "factor", "factor")


# metadata <- data.frame('year'=c(2011), 'nrow'=nrow(I))

for(yr in (2012:2016)){
  i_file <- paste("../csv/",yr, "_i.csv", sep="")
  
  # I <- rbind(I, read.csv(i_file, header=T))
  i_df <- read.csv(i_file, header=T, stringsAsFactors = F)
  # metadata <- rbind(metadata, c(yr, nrow(i_df)))
  I <- rbind(I, i_df)
  
  # i_df <- NULL # keeps the environment tidy
  rm(i_df)
}


# I$ARRIVE_DATE <- as.POSIXct(I$ARRIVE_DATE)
# str(I)

# sapply(I, summary)

```


### Input Data: Responding Units
```{r}
## Responding Units
# Load & Combine Dataset from Yearwise .csv

RU <- read.csv("../csv/2011_ru.csv", header=T, stringsAsFactors = F, na.strings=c("","NA"))


for(yr in (2012:2016)){
  r_file <- paste("../csv/",yr, "_ru.csv", sep="")
  
  r_df <- read.csv(r_file, header=T, stringsAsFactors = F)
  RU <- rbind(RU, r_df)
  r_df <- NULL # keeps the environment tidy
}


```

## Working with R Data Types
### Dates

```{r}
## Dates
# In this section I work with the three date features: DISPATCH_DATE, ARRIVE_DATE & INCIDENT_DATE
# In fact, DISPATCH_DATE & ARRIVE_DATE are identical so I drop one (arbitrary choice: ARRIVE_DATE)

# This is some of the initial exploring of the dates I did before noticing that DISPATCH & ARRIVE dates were identical.
# I've left the code here for observation. It's commented out to save computation time & resources

# nrow(I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),]) #16802
# nrow(I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$ARRIVE_DATE),]) #16802
# nrow(I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$ARRIVE_DATE) & !grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),])

# Originally I was reading characters in from the .csv as factors. I changed this to reduce overhead. 
# I$DISPATCH_DATE <- as.character(I$DISPATCH_DATE)
# I$ARRIVE_DATE <- as.character(I$ARRIVE_DATE)
# I$INCINCIDENT_DATE <- as.character(I$INCIDENT_DATE) 

# I[,c('DISPATCH_DATE','ARRIVE_DATE')] <- as.character(I[,c('DISPATCH_DATE','ARRIVE_DATE')]) # Slow

# I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$ARRIVE_DATE) & !grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),'DISPATCH_DATE'] <- NA
# I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$ARRIVE_DATE) & !grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),'ARRIVE_DATE'] <- NA
# I[!grepl("\\d{2}/\\d{2}/\\d{4}", I$INCIDENT_DATE),'INCIDENT_DATE'] <- NA # Everyone obs. has a value

# nrow(I[!grepl("\\d{2}/\\d{2}/\\d{4}", I$INCIDENT_DATE),]) # 0



if(identical(I$DISPATCH_DATE, I$ARRIVE_DATE)){ I$ARRIVE_DATE <- NULL }

I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),'DISPATCH_DATE'] <- NA
# Cannot set to date type because of 16,802 Missing Values
I$DISPATCH_DATE <- as.POSIXct(I$DISPATCH_DATE)
# I$ARRIVE_DATE <- as.POSIXct(I$ARRIVE_DATE) # I set I$ARRIVE_DATE to null, above
I$INCIDENT_DATE <- as.POSIXct(I$INCIDENT_DATE, format="%d/%m/%Y")
# td <- as.Date(I$INCIDENT_DATE, format="%d/%m/%Y")

sapply(I[,c('DISPATCH_DATE','INCIDENT_DATE')], function(x) sum(is.na(x)))
# sapply(I, function(x) sum(is.na(x))/nrow(I))
```

### More Dates
The above date & time section was among the first aspects of this data set that I explored. I discovered some relationships among the dates that were useful for my circular Coxwell plots. I also used that information to develop the 'TTA' continuous attribute (among the few continuous features in this data set). However, later on I found a good use for inididual units of date-time variables (such as having hour, minutes and seconds their own features): Association Rules. I hope to incorporate temporal data into my final model because it is one reliable datum available at call time. 
```{r}
# INCIDENT_DAY is a useful feature for graphing & summary statistics. Convenient to have
# the levels in order. 
I$INCIDENT_DAY <- factor(x = format(I$INCIDENT_DATE, '%A'), levels=c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))

I$INCIDENT_MONTH <- factor(x = format(I$INCIDENT_DATE, '%B'),
                           levels = c('January','February','March','April','May','June','July',
                                      'August', 'September','November','December'))

```


### Factors
There are many categorical features in this dataset. I decided to encode them as factors. Empty & blank values are treated as NA. 

```{r}
# This was a tricky part of the data prep to handle. This is a relatively sparse dataset (see below for NA quantities!)
# but also has a lot of factors. I decided to keep the NAs as a level in the factors to illustrate restrictions
# in the analysis introduced by such sparse data. 
factorSchema <- c("EVENT_TYPE","EVENT_TYPE_CD","MAIN_STREET","CROSS_STREET","FSA","ALARM_TO_FD","RESPONSE_TYPE","STATUS_ON_ARRIVAL","WATER","FIRE_CONTROL","PROPERTY","AREA_OF_ORIGIN","IGNITION_SOURCE","FUEL_OF_IGNITION_SOURCE","OBJECT_OR_MATERIAL_FIRST_IGNITED","POSSIBLE_CAUSE","VEH_PURPOSE","VEH_FUEL","INSURANCE_ESTIMATE","EST_VALUE_AT_RISK","PHYSICAL_CONDITION_1","PHYSICAL_CONDITION_2","PHYSICAL_CONDITION_3","CIV_FIRE_CONTROL","CIV_EVACUATION","CIV_EVACUATION_REASON_1","CIV_EVACUATION_REASON_2","OPP","MOE","TSSA","ESA","MOL","EMS","CANUTEC","GAS","HYDRO","MUNICIPAL_BUILDING_OFFICE","MUNICIPAL_HEALTH_OFFICE","MUNICIPAL_POLICE","OTHER","INITIAL_DETECTION","EXTENT_FIRE","EXTENT_SMOKE","POSSIBLE_BUSINESS_IMPACT","OCC_STATUS","OCC_TYPE","BLD_STATUS","BLD_HEIGHT","LEVEL_OF_ORIGIN","AGE_OF_STRUCTURE","SMOKE_ALARM_PRESENCE_AND_OPERATION_MAIN_FLOOR","SMOKE_ALARM_FAILURE_TO_OPERATE","SMOKE_ALARM_TYPE","SMOKE_ALARM_OTHER_FLOOR_PRESENCE","SMOKE_ALARM_ON_ALL_FLOORS","SMOKE_ALARM_IMPACT_ON_EVAC","FIRE_ALARM_SYSTEM_PRESENCE","FIRE_ALARM_SYSTEM_OPERATION","FIRE_ALARM_SYSTEM_IMPACT","SPRINKLER_SYSTEM_PRESENCE","SPRINKLER_SYSTEM_ACTIVATION")

# grepl(".*\\S.*", "   ") # True if there's anything besides whitespace. 

for(ft in factorSchema){
  I[!grepl(".*\\S.*", I[,ft]),ft] <- NA
  I[,ft] <- addNA(I[,ft]) # creates factors with NA level
}
```

### Counting NAs
```{r}
# This sapply returns column NAs as percentage of total
# but doesn't format the output as nicely as the below function which I kept. 
# 
# sapply(I, function(x){
#   per <- sum(is.na(x))/nrow(I)
#   if(per * 100 > 0.001) return(per*100)
#   return(0)
# })


# outputs number of NAs per column as a percent
# format & round because big floats being returned & hard to read.
sapply(I, function(x){
  
  p <- sum(is.na(x))/nrow(I)
  format(round(p*100, 2), nsmall=2)
})

# sapply(I, function(x) sum(is.na(x)))
# apply(I, 2, function(x) sum(is.na(x))/nrow(I))
# apply(I, 1, function(x) sum(is.na(x))/ncol(I))

I$ROW_NAS <- apply(I, 1, function(x) sum(is.na(x)))

summary(I$ROW_NAS)
summary(I)
# I[!grepl(".*\\S.*", I[,ft]),ft] <- NA
# sum(!grepl(".*\\S.*", I$FD_STATION))
```
### More work on NAs
One thing I have noticed is that quite a few columns have exactly the same number of NAs.
What types of incidents are these?

After an initial glance these incidents seem to be mostly fires. 

```{r}
NA_cols <- c('COMPLEX','OCC_STATUS','OCC_type','BLD_STATUS','BLD_HEIGHT','LEVEL_OF_ORIGIN','AGE_OF_STRUCTURE','SMOKE_ALARM_PRESENCE_AND_OPERATION_MAIN_FLOOR','SMOKE_ALARM_FAILURE_TO_OPERATE','SMOKE_ALARM_TYPE','SMOKE_ALARM_OTHER_FLOOR_PRESENCE','SMOKE_ALARM_ON_ALL_FLOORS','SMOKE_ALARM_IMPACT_ON_EVAC','FIRE_ALARM_SYSTEM_PRESENCE','FIRE_ALARM_SYSTEM_OPERATION','FIRE_ALARM_SYSTEM_IMPACT','SPRINKLER_SYSTEM_PRESENCE','SPRINKLER_SYSTEM_ACTIVATION')

FIRES <- I[
  !is.na(I$COMPLEX) &
  !is.na(I$OCC_STATUS) &
  !is.na(I$OCC_TYPE) &
  !is.na(I$BLD_STATUS) &
  !is.na(I$BLD_HEIGHT) & 
  !is.na(I$LEVEL_OF_ORIGIN) &
  !is.na(I$AGE_OF_STRUCTURE) &
  !is.na(I$SMOKE_ALARM_PRESENCE_AND_OPERATION_MAIN_FLOOR) &
  !is.na(I$SMOKE_ALARM_FAILURE_TO_OPERATE) &
  !is.na(I$SMOKE_ALARM_TYPE) &
  !is.na(I$SMOKE_ALARM_ON_ALL_FLOORS) &
  !is.na(I$SMOKE_ALARM_IMPACT_ON_EVAC) &
  !is.na(I$FIRE_ALARM_SYSTEM_PRESENCE) &
  !is.na(I$FIRE_ALARM_SYSTEM_IMPACT) &
  !is.na(I$SPRINKLER_SYSTEM_PRESENCE) &
  !is.na(I$SPRINKLER_SYSTEM_ACTIVATION)
,]

table(FIRES$PROPERTY_GROUP)/length(FIRES$PROPERTY_GROUP)
table(I$PROPERTY_GROUP)/length(I$PROPERTY_GROUP)


```

## Feature Selection
### INCIDENT_NUMBER & FD_STATION
As I continue my exploratory data analysis I keep finding features that appear, at first glance, to be completely redundant. A prime example of such a feature is FD_STATION: The below call to pbapply returns 720340. Only 30 entries don't correspond to this pattern! Of that 30, 6 are NA and the other 24 are simply blank strings in the INCIDENT_NUMBER field.


```{r}
sum(substring(I$FD_STATION, 0, nchar(I$INCIDENT_NUMBER)) == I$INCIDENT_NUMBER | I$FD_STATION == '0', na.rm=T) #720364

# sum(as.numeric(gsub(I$INCIDENT_NUMBER, "", I$FD_STATION)) == 0, na.rm=T)
sum(pbapply(I[,c('INCIDENT_NUMBER','FD_STATION')],1, function(x){
  as.numeric(gsub(x['INCIDENT_NUMBER'],'',x['FD_STATION'])) == 0
}), na.rm=T) #returns 720340

in_eq_fdstn <- pbapply(I[,c('INCIDENT_NUMBER','FD_STATION')],1, function(x){
  as.numeric(gsub(x['INCIDENT_NUMBER'],'',x['FD_STATION'])) == 0
})

# As FD_STATION is a bit pointless I'm dropping it.
I$FD_STATION <- NULL
```


### Some preliminary cross-tables

```{r}
# https://www.stat.berkeley.edu/~s133/dates.html
# Some cool temporal breakdowns
table(format(I$INCIDENT_DATE, '%Y')) 
table(format(I$INCIDENT_DATE, '%A'))
table(format(I$INCIDENT_DATE, '%B'))
table(format(I$DISPATCH_DATE, '%H'))
table(I$PROPERTY)
# INCIDENT_NUMBER & FD_STATION have a lot of similarities...
sum(I$INCIDENT_NUMBER == substr(I$FD_STATION,0,9), na.rm=T) #375456
length(unique(I$FD_STATION)) #375478
```

### Grouping Property Type by Category

```{r}
# https://www.quora.com/How-do-I-get-a-frequency-count-based-on-two-columns-variables-in-an-R-dataframe?share=1

# I'm trying to build a cross-table

I$PROPERTY_GROUP <- vector(mode='character', length=nrow(I))
# I[I$PROPERTY]
PROPERTY_GROUP <- list()
PROPERTY_GROUP$A <- (101:199)
PROPERTY_GROUP$B <- (201:299)
PROPERTY_GROUP$C <- (301:399)
PROPERTY_GROUP$D <- (401:499)
PROPERTY_GROUP$E <- (501:599)
PROPERTY_GROUP$F <- (601:799)
PROPERTY_GROUP$O <- (801:999)

for(g in c(LETTERS[(1:6)],'O')){
  I$PROPERTY_GROUP[I$PROPERTY %in% PROPERTY_GROUP[[g]]] <- g
}

I[I$PROPERTY_GROUP == "", 'PROPERTY_GROUP'] <- NA
# Would you like this feature as a factor?
I$PROPERTY_GROUP <- addNA(I$PROPERTY_GROUP)
# Would you like it to be descriptively named?
levels(I$PROPERTY_GROUP) <- c('Assembly','Care & Detention','Residential','Business & Personal Services','Mercantile','Industrial','Not Classified',NA)



table(format(I$INCIDENT_DATE, '%A'), I$PROPERTY_GROUP) # This is Day of week vs Property Group
```
### Event type & group
The approach here was to combine similar-seeming event types into their own, larger group category. 
```{r}
# Firstly, EVENT_TYPE has at least 3 mistakes: 31 & 33 aren't listed as possibilities so I'm setting them to NA
I[I$ALARM_TO_FD %in% 33, "ALARM_TO_FD"] <- NA
I[I$ALARM_TO_FD %in% 31, "ALARM_TO_FD"] <- NA
# levels(I$ALARM_TO_FD) <- c(as.character(1:11), NA)
I$ALARM_TO_FD <- droplevels(I$ALARM_TO_FD)

event_typ_grp <- read.csv(file="../csv/event_type.csv", header=T)
# Below dirty one-liner doesn't seem to work... perhaps a mystery for another day.
# I$EVENT_GROUP <- merge(I, event_typ_grp, by="EVENT_TYPE")[,'EVENT_GROUP'] # Lefts inner join

# the slower, less l33t approach:
I <- merge(I, event_typ_grp, by="EVENT_TYPE")
I$Freq <- NULL
# I <- cbind(I, It[,'EVENT_GROUP'])
# I$EVENT_GROUP <- factor(It$EVENT_GROUP)

```

### Response Group
As with EVENT_TYPE and PROPERTY_TYPE, I believe grouping RESPONSE_TYPE would be useful.
```{r}
I$RESPONSE_GROUP <- vector(mode="character", length=nrow(I))

I[I$RESPONSE_TYPE %in% 1:3, 'RESPONSE_GROUP'] <- "A"
I[I$RESPONSE_TYPE %in% 11:13, 'RESPONSE_GROUP'] <- "B"
I[I$RESPONSE_TYPE %in% 21:29, 'RESPONSE_GROUP'] <- "C"
I[I$RESPONSE_TYPE %in% c(23,36), 'RESPONSE_GROUP'] <- "D"
I[I$RESPONSE_TYPE %in% c(31:35, 39), 'RESPONSE_GROUP'] <- "E"
I[I$RESPONSE_TYPE %in% 37:38, 'RESPONSE_GROUP'] <- "F"
I[I$RESPONSE_TYPE %in% c(53,41:51,54,57:59), 'RESPONSE_GROUP'] <- "G"
I[I$RESPONSE_TYPE %in% c(61:69, 601:605,698,699), 'RESPONSE_GROUP'] <- "H"
I[I$RESPONSE_TYPE %in% c(701:703,71,73:76,82,84,85,86,88,89,898,899), 'RESPONSE_GROUP'] <- "I"
I[I$RESPONSE_TYPE %in% c(921,922,910:913,92:99), 'RESPONSE_GROUP'] <- "J"

```



### Response Time (Time-to-arrival)

For this 'new' feature I subtracted response time, or Time-to-arrival, by subtracting the ONSCENE_TIME from the INITIAL_CALL_TIME, both of which were fairly complete in the dataset. I found that the dataset was encoded such that INCIDENT_DATE was always the date of the initial call. For cases where the call was received late at night, say 11:59pm, and the Responding Units didn't arrive until the next day I added one day (24 * 60 * 60 seconds) to the INCIDENT_DATE used for ONSCENE TIME.

I wrote a function to make this calculation, below, and passed it the rows of the dataset, one at a time, using apply. This process took some time to compute on my mid 2009 MacBook. Originally approximately 25 minutes, I was able to reduce it to 15 minutes by simplifying the function a bit and also only passing the relevant columns to apply.

```{r}
MEDICAL <- I[I$EVENT_TYPE %in% 'Medical',]
table(format(MEDICAL$INCIDENT_DATE, '%A'), MEDICAL$PROPERTY_GROUP)

# This below function calculates time difference between time call was received
# and recorded onscene time. I accounted for cases where the date would roll over between call & arrival.
# However, I later realized that the data itself is encoded to account for this: 
# DISPATCH_DATE includes the date & time of ONSCENE whereas INCIDENT_DATE is the date of call. 


tta <- function(x){
  if(sum(is.na(c(x["INCIDENT_DATE"], x["INITIAL_CALL_HOUR"], x["INITIAL_CALL_MIN"], x["INITIAL_CALL_SEC"], x["ONSCENE_HOUR"],x["ONSCENE_MIN"],x["ONSCENE_SEC"]))) > 0) return(NA)
  # onscene_date <- x["INCIDENT_DATE"]
  ic <- as.POSIXct(paste0(
    x["INCIDENT_DATE"], " ", x["INITIAL_CALL_HOUR"], ":", x["INITIAL_CALL_MIN"], ":", x["INITIAL_CALL_SEC"] 
  ), format="%Y-%m-%d %H:%M:%S", tz="EST")
  os <- as.POSIXct(paste0(
    x["INCIDENT_DATE"], " ", x["ONSCENE_HOUR"], ":", x["ONSCENE_MIN"], ":", x["ONSCENE_SEC"]
  ), format="%Y-%m-%d %H:%M:%S", tz="EST")
  if(x["ONSCENE_HOUR"] < x["INITIAL_CALL_HOUR"]){ os <- os + (24 * 60 * 60) }
  return(as.numeric(os) - as.numeric(ic))
  
}




# I[!is.na(I$INITIAL_CALL_HOUR) & !is.na(I$ONSCENE_HOUR) & I$ONSCENE_HOUR < I$INITIAL_CALL_HOUR,] #Rollover dispatch
# nrow(I[!is.na(I$INITIAL_CALL_HOUR) & !is.na(I$ONSCENE_HOUR) & I$ONSCENE_HOUR < I$INITIAL_CALL_HOUR,]) #2519

# Below is the code to actually create the 'TTA' column: First an empty numeric vector, later populated with the call to apply(). This call took about 15 minutes to run. I used pbapply, from the pabbly package, for the progress bar  (very handy!)
#
# library(pbapply)
# I$TTA <- vector(length=nrow(I), mode="numeric")
# I$TTA <- pbapply(I[,c('INCIDENT_DATE','INITIAL_CALL_HOUR','INITIAL_CALL_MIN','INITIAL_CALL_SEC','ONSCENE_HOUR','ONSCENE_MIN','ONSCENE_SEC')], 1, tta)
# 
# Since I have already calculated the TTA values I save I save time when loading the project by either saving the .RData or writing the columns as a .csv and re-loading the TTA feature back into I. 
# write.csv(x=I[,c('INCIDENT_NUMBER','TTA')], file="TTA.csv")
# TTA <- read.csv(file="../csv/TTA.csv", header = T)
# I <- cbind(I, TTA$TTA)
# names(I)[101] <- "TTA"

# Now that we have TTA we can do some cool cross-tables
tapply(I$TTA, I$PROPERTY_GROUP, mean, na.rm=T)

# Trying to test the above: Boolean "is DISPATCH_DATE" time always ONSCENE time?
sum(format(I$DISPATCH_DATE, format='%Y-%m-%d') == I$INCIDENT_DATE, na.rm=T) # 701044
sum(I$INITIAL_CALL_HOUR > I$ONSCENE_HOUR, na.rm=T) # 2519
nrow(I[!is.na(I$DISPATCH_DATE) & format(I$DISPATCH_DATE, format='%Y-%m-%d') != I$INCIDENT_DATE,]) # 2524

# There seem to be a few observations (17) that don't fit the pattern:
# Incident Date: Date initial call was received
# Dispatch Date: Date & Time first responders arrive on scene
# I'm guessing that these cases are a result of a data input error. After briefly browsing aforementioned 17 records, 
# I noticed some of the fields don't match up: Control date, when inputted, close to incident or dispach date.. Also the times used tend to be all over the place and don't correspond to other columns. 
# I might remove these observations entirely. 

nrow(I[!is.na(I$DISPATCH_DATE) & format(I$DISPATCH_DATE, format='%Y-%m-%d') != I$INCIDENT_DATE & I$INITIAL_CALL_HOUR <= I$ONSCENE_HOUR,]) # 17
# total obs - (# dispatch date = incident date) - (#edge cases) - (Dispatch Date NAs) 
# = 720370 - 701044 - 2519 - 16802  

# The below lines are to calculate edge cases. A Date is said to be "less than" another date if it occurred before the date being compared to. Example: January 1st, 1980 < January 2nd, 1980. This is easier to remember if you think of dates in terms of 'Unix seconds' wheras the integer value equality holds. 
# 
sum(I$DISPATCH_DATE > I$INCIDENT_DATE, na.rm=T) # 703568
sum(is.na(I$DISPATCH_DATE > I$INCIDENT_DATE)) # 16802
sum(I[!is.na(I$DISPATCH_DATE), 'DISPATCH_DATE'] > I[!is.na(I$DISPATCH_DATE), 'INCIDENT_DATE'])
sum(is.na(I$DISPATCH_DATE)) + sum(I[!is.na(I$DISPATCH_DATE), 'DISPATCH_DATE'] > I[!is.na(I$DISPATCH_DATE), 'INCIDENT_DATE']) == nrow(I)

```

```{r}
aggregate(I$TTA, list(I$PROPERTY_GROUP), mean, na.rm=T)
table(I$PROPERTY_GROUP)
summary(I)
sum(I$TTA == 0, na.rm = T)
```
### More Cross Tables
I am interested in the relationship between EVENT_TYPE & ALARM TO FD
```{r}
# https://stats.stackexchange.com/questions/81483/warning-in-r-chi-squared-approximation-may-be-incorrect
# chisq.test below gives warning "Chi-squared approximation may be incorrect" which is a bit
# disheartening. The above post on stackexchange advises to set simulate.p.value = T in the call to the test function. 

et_vs_atofd <- addmargins(table(I$EVENT_TYPE, I$ALARM_TO_FD), 2)
# table(I[!I$ALARM_TO_FD %in% NA, 'EVENT_GROUP'], droplevels(I[!I$ALARM_TO_FD %in% NA, 'ALARM_TO_FD']))
EVENT_GROUP <- droplevels(I[!I$ALARM_TO_FD %in% NA & !I$EVENT_GROUP %in% NA, 'EVENT_GROUP'])
ALARM_TO_FD <- droplevels(I[!I$ALARM_TO_FD %in% NA & !I$EVENT_GROUP %in% NA, 'ALARM_TO_FD'])
table(EVENT_GROUP, ALARM_TO_FD)
# I think that having so many 0s in ALARM_TO_FD 10 & 11 is giving me df = NA for chisq.test. 
# As a result I have opted to merge them and run the test again...
# Merging makes sense because the OFM Code List has both 10 & 11 as "No alarm received"

levels(ALARM_TO_FD) <- c(as.character(1:10),10)

# chisq.test(table(I$EVENT_GROUP, I$ALARM_TO_FD), simulate.p.value = T) # returns df = NA
chisq.test(table(I[!I$ALARM_TO_FD %in% NA, 'EVENT_GROUP'], droplevels(I[!I$ALARM_TO_FD %in% NA, 'ALARM_TO_FD'])), simulate.p.value = T) # Also has df = NA. I believe there were some NAs that weren't removed from EVENT_GROUP
chisq.test(table(EVENT_GROUP, ALARM_TO_FD), simulate.p.value = T)

```

### Low Variance Filter
```{r}
sum(I$FF_FATALITIES > 0) # 0
I$FF_FATALITIES <- NULL

sum(I$EST_LOSS > 0)
I$EST_LOSS_BINS <- cut(I$EST_LOSS, c(0,5000,10000,max(I$EST_LOSS)), include.lowest=T)

```

## Association Rules
```{r}
# library(arules)
# below won't run because apriori() only appreciataes factors or logical...
rules <- apriori(I[,c('EVENT_GROUP','ALARM_TO_FD','RESPONSE_GROUP','PROPERTY_GROUP','INCIDENT_MONTH','INCIDENT_DAY','INITIAL_CALL_HOUR','EVENT_TYPE_CD')], parameter = list(supp = 0.5, conf = 0.9, target="rules"))

rules <- apriori(data.frame(
  EVENT_GROUP = I$EVENT_GROUP,
  ALARM_TO_FD = I$ALARM_TO_FD,
  RESPONSE_GROUP = addNA(I$RESPONSE_GROUP),
  PROPERTY_GROUP = addNA(I$PROPERTY_GROUP),
  MONTH = I$INCIDENT_MONTH,
  DAY = I$INCIDENT_DAY,
  HOUR = addNA(I$INITIAL_CALL_HOUR),
  EVENT_TYPE_CD = I$EVENT_TYPE_CD
 ),
 parameter = list(supp = 0.1, conf = 0.1, target = "rules")
 )

rules <- apriori(data.frame(
  EVENT_TYPE = I$EVENT_TYPE,
  ALARM_TO_FD = I$ALARM_TO_FD,
  # RESPONSE_GROUP = addNA(I$RESPONSE_GROUP),
  # PROPERTY_GROUP = addNA(I$PROPERTY_GROUP),
  RESPONSE_TYPE = I$RESPONSE_TYPE,
  PROPERTY_TYPE = I$PROPERTY,
  MONTH = I$INCIDENT_MONTH,
  DAY = I$INCIDENT_DAY,
  HOUR = addNA(I$INITIAL_CALL_HOUR),
  EVENT_TYPE_CD = I$EVENT_TYPE_CD
 ),
 parameter = list(supp = 0.5, conf = 0.5, target = "rules")
 )
```


## Some Graphics
```{r}
# png('out.png')
barplot(table(format(I$INCIDENT_DATE, '%Y')), main="Incidents by Year", xlab="Year", ylab="Number of Incidents")
#dev.off() # saves the file. Important!

ggplot(data = I, mapping=aes(x = format(I$INCIDENT_DATE, '%Y'))) + geom_bar() + labs(x = "Year", y = 'Total Incidents', title = 'Incident Count by Year')

# Courly Coxwell plot of Event count 
ggplot(data = I, mapping=aes(x=INITIAL_CALL_HOUR)) + geom_bar() + labs(x = NULL, y = NULL, title="All Event Hours") + scale_x_continuous(breaks = 0:23) + coord_polar() + theme(plot.title = element_text(hjust = 0.5))

# Hourly Coxwell faceted by day of week 
ggplot(data = I, mapping=aes(x = INITIAL_CALL_HOUR)) + geom_bar() + facet_wrap( ~ INCIDENT_DAY, nrow=2) + coord_polar() + labs(x = NULL, y = NULL, title="Daily breakdown of event hours")

# Vertical barplot of Incidents by Event Group. Requires scales
# library(scales)
ggplot(data = I, mapping=aes(x = EVENT_GROUP)) + geom_bar() + scale_x_discrete(limits = rev(levels(I$EVENT_GROUP))) + scale_y_continuous(labels = comma) + coord_flip() + labs(x = "Incident Count", y = "Event Group", title="Incident Count by Event Group")

# Event Group portions of call source
ggplot(data = I, mapping=aes(x = EVENT_GROUP, fill=ALARM_TO_FD)) + geom_bar(position="fill") + theme_minimal()

# Incident Count by Property Group
# Note that this was created after renaming levels in I$PROPERTY_GROUP
ggplot(data = I[!I$PROPERTY_GROUP %in% NA,], mapping=aes(x = PROPERTY_GROUP)) + geom_bar() + scale_y_continuous(labels = comma)+ theme(axis.text.x = element_text(angle = 45, hjust=1)) + scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + labs(x = "Property Group", y = "Total Incidents", title="Incident Count by Property Group")

# Having Event Group on the X Axis provides a bit more information (I think?)
levels(I$ALARM_TO_FD) <- c('911','Civilian Telephone','Ambulance','Police Services','Monitoring Agency','Direct Connection','In Person','FD Radio','Other','No Alarm','FD Discovery', NA)
ggplot(I[!is.na(I$EVENT_GROUP) & !is.na(I$ALARM_TO_FD), ], mapping=aes(x = EVENT_GROUP, fill=ALARM_TO_FD)) + geom_bar(position = "fill") + scale_y_continuous(labels = comma) + theme(axis.text.x = element_text(angle = 45, hjust=1)) + scale_x_discrete(labels = function(x) str_wrap(x, width = 10))

```

