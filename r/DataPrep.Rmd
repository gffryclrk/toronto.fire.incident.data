---
title: "CMTH642 Capstone"
author: "Geoffrey Clark"
date: "May 28, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Fire Incident Data Overview
### Input Data: Incidents
```{r}
## Incidents
# Load & Combine Dataset from Yearwise .csv

I <- read.csv("../csv/2011_i.csv", header=T, stringsAsFactors = F, na.strings=c("","NA", " "))
# importSchema = c("character", "factor", "POSIXct", "POSIXct", "factor", "factor", "factor", "factor", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "character", "factor", "factor", "character", "character", "character", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "character", "factor", "factor", "character", "character", "character", "character", "character", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "character", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "factor", "character", "factor", "factor", "factor", "factor", "factor")


# metadata <- data.frame('year'=c(2011), 'nrow'=nrow(I))

for(yr in (2012:2016)){
  i_file <- paste("../csv/",yr, "_i.csv", sep="")
  
  # I <- rbind(I, read.csv(i_file, header=T))
  i_df <- read.csv(i_file, header=T, stringsAsFactors = F)
  # metadata <- rbind(metadata, c(yr, nrow(i_df)))
  I <- rbind(I, i_df)
  
  # i_df <- NULL # keeps the environment tidy
  rm(i_df)
  rm(i_file)
}


# I$ARRIVE_DATE <- as.POSIXct(I$ARRIVE_DATE)
# str(I)

# sapply(I, summary)

```


### Input Data: Responding Units
```{r}
## Responding Units
# Load & Combine Dataset from Yearwise .csv

RU <- read.csv("../csv/2011_ru.csv", header=T, stringsAsFactors = F, na.strings=c("","NA"))


for(yr in (2012:2016)){
  r_file <- paste("../csv/",yr, "_ru.csv", sep="")
  
  r_df <- read.csv(r_file, header=T, stringsAsFactors = F)
  RU <- rbind(RU, r_df)
  r_df <- NULL # keeps the environment tidy
}


```

## Working with R Data Types
### Dates

```{r}
## Dates
# In this section I work with the three date features: DISPATCH_DATE, ARRIVE_DATE & INCIDENT_DATE
# In fact, DISPATCH_DATE & ARRIVE_DATE are identical so I drop one (arbitrary choice: ARRIVE_DATE)

# This is some of the initial exploring of the dates I did before noticing that DISPATCH & ARRIVE dates were identical.
# I've left the code here for observation. It's commented out to save computation time & resources

# nrow(I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),]) #16802
# nrow(I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$ARRIVE_DATE),]) #16802
# nrow(I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$ARRIVE_DATE) & !grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),])

# Originally I was reading characters in from the .csv as factors. I changed this to reduce overhead. 
# I$DISPATCH_DATE <- as.character(I$DISPATCH_DATE)
# I$ARRIVE_DATE <- as.character(I$ARRIVE_DATE)
# I$INCINCIDENT_DATE <- as.character(I$INCIDENT_DATE) 

# I[,c('DISPATCH_DATE','ARRIVE_DATE')] <- as.character(I[,c('DISPATCH_DATE','ARRIVE_DATE')]) # Slow

# I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$ARRIVE_DATE) & !grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),'DISPATCH_DATE'] <- NA
# I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$ARRIVE_DATE) & !grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),'ARRIVE_DATE'] <- NA
# I[!grepl("\\d{2}/\\d{2}/\\d{4}", I$INCIDENT_DATE),'INCIDENT_DATE'] <- NA # Everyone obs. has a value

# nrow(I[!grepl("\\d{2}/\\d{2}/\\d{4}", I$INCIDENT_DATE),]) # 0



if(identical(I$DISPATCH_DATE, I$ARRIVE_DATE)){ I$ARRIVE_DATE <- NULL }

I[!grepl("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}", I$DISPATCH_DATE),'DISPATCH_DATE'] <- NA
# Cannot set to date type because of 16,802 Missing Values
I$DISPATCH_DATE <- as.POSIXct(I$DISPATCH_DATE)
# I$ARRIVE_DATE <- as.POSIXct(I$ARRIVE_DATE) # I set I$ARRIVE_DATE to null, above
I$INCIDENT_DATE <- as.POSIXct(I$INCIDENT_DATE, format="%d/%m/%Y")
# td <- as.Date(I$INCIDENT_DATE, format="%d/%m/%Y")

sapply(I[,c('DISPATCH_DATE','INCIDENT_DATE')], function(x) sum(is.na(x)))
# sapply(I, function(x) sum(is.na(x))/nrow(I))


# Control Date
# I$CONTROL_DATE has many empty values that should probably be set to NA
# sum(nchar(I$CONTROL_DATE) == 0, na.rm=T) #335564
I$CONTROL_DATE[I$CONTROL_DATE == ""] <- NA


```

### More Dates
The above date & time section was among the first aspects of this data set that I explored. I discovered some relationships among the dates that were useful for my circular Coxwell plots. I also used that information to develop the 'TTA' continuous attribute (among the few continuous features in this data set). However, later on I found a good use for inididual units of date-time variables (such as having hour, minutes and seconds their own features): Association Rules. I hope to incorporate temporal data into my final model because it is one reliable datum available at call time. 
```{r}
# INCIDENT_DAY is a useful feature for graphing & summary statistics. Convenient to have
# the levels in order. 
I$INCIDENT_DAY <- factor(x = format(I$INCIDENT_DATE, '%A'), levels=c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))

I$INCIDENT_MONTH <- factor(x = format(I$INCIDENT_DATE, '%B'),
                           levels = c('January','February','March','April','May','June','July',
                                      'August', 'September','October','November','December'))

# I$INCIDENT_MONTH <- format(I$INCIDENT_DATE, '%B')
# I$INCIDENT_MONTH
I$INCIDENT_YEAR <- factor(x = format(I$INCIDENT_DATE, '%Y'), levels=c(2011,2012,2013,2014,2015,2016))
```


### Factors
There are many categorical features in this dataset. I decided to encode them as factors. Empty & blank values are treated as NA. 

```{r}
# This was a tricky part of the data prep to handle. This is a relatively sparse dataset (see below for NA quantities!)
# but also has a lot of factors. I decided to keep the NAs as a level in the factors to illustrate restrictions
# in the analysis introduced by such sparse data. 
factorSchema <- c("EVENT_TYPE","EVENT_TYPE_CD","MAIN_STREET","CROSS_STREET","FSA","ALARM_TO_FD","RESPONSE_TYPE","STATUS_ON_ARRIVAL","WATER","FIRE_CONTROL","PROPERTY","AREA_OF_ORIGIN","IGNITION_SOURCE","FUEL_OF_IGNITION_SOURCE","OBJECT_OR_MATERIAL_FIRST_IGNITED","POSSIBLE_CAUSE","VEH_PURPOSE","VEH_FUEL","INSURANCE_ESTIMATE","EST_VALUE_AT_RISK","PHYSICAL_CONDITION_1","PHYSICAL_CONDITION_2","PHYSICAL_CONDITION_3","CIV_FIRE_CONTROL","CIV_EVACUATION","CIV_EVACUATION_REASON_1","CIV_EVACUATION_REASON_2","OPP","MOE","TSSA","ESA","MOL","EMS","CANUTEC","GAS","HYDRO","MUNICIPAL_BUILDING_OFFICE","MUNICIPAL_HEALTH_OFFICE","MUNICIPAL_POLICE","OTHER","INITIAL_DETECTION","EXTENT_FIRE","EXTENT_SMOKE","POSSIBLE_BUSINESS_IMPACT","OCC_STATUS","OCC_TYPE","BLD_STATUS","BLD_HEIGHT","LEVEL_OF_ORIGIN","AGE_OF_STRUCTURE","SMOKE_ALARM_PRESENCE_AND_OPERATION_MAIN_FLOOR","SMOKE_ALARM_FAILURE_TO_OPERATE","SMOKE_ALARM_TYPE","SMOKE_ALARM_OTHER_FLOOR_PRESENCE","SMOKE_ALARM_ON_ALL_FLOORS","SMOKE_ALARM_IMPACT_ON_EVAC","FIRE_ALARM_SYSTEM_PRESENCE","FIRE_ALARM_SYSTEM_OPERATION","FIRE_ALARM_SYSTEM_IMPACT","SPRINKLER_SYSTEM_PRESENCE","SPRINKLER_SYSTEM_ACTIVATION")

# grepl(".*\\S.*", "   ") # True if there's anything besides whitespace. 

for(ft in factorSchema){
  I[!grepl(".*\\S.*", I[,ft]),ft] <- NA
  I[,ft] <- addNA(I[,ft]) # creates factors with NA level
}

rm(factorSchema) # Trying to keep my environment somewhat tidy...

# Additional factors which I didn't originally encode properly: (I don't feel like re-ininitalizing my environment right now...)
I$OFM_INVESTIGATIONS_CONTACTED <- addNA(I$OFM_INVESTIGATIONS_CONTACTED)
I$AID_TO_FROM_OTHER_DEPTS <- addNA(I$AID_TO_FROM_OTHER_DEPTS)
I$COMPLEX <- addNA(I$COMPLEX)
```

### Counting NAs
```{r}
# This sapply returns column NAs as percentage of total
# but doesn't format the output as nicely as the below function which I kept. 
# 
# sapply(I, function(x){
#   per <- sum(is.na(x))/nrow(I)
#   if(per * 100 > 0.001) return(per*100)
#   return(0)
# })


# outputs number of NAs per column as a percent
# format & round because big floats being returned & hard to read.
sapply(I, function(x){
  
  p <- sum(is.na(x))/nrow(I)
  format(round(p*100, 2), nsmall=2)
})

# sapply(I, function(x) sum(is.na(x)))
# apply(I, 2, function(x) sum(is.na(x))/nrow(I))
# apply(I, 1, function(x) sum(is.na(x))/ncol(I))

I$ROW_NAS <- apply(I, 1, function(x) sum(is.na(x)))

summary(I$ROW_NAS)
summary(I)
# I[!grepl(".*\\S.*", I[,ft]),ft] <- NA
# sum(!grepl(".*\\S.*", I$FD_STATION))
```
### More work on NAs
One thing I have noticed is that quite a few columns have exactly the same number of NAs.
What types of incidents are these?

After an initial glance these incidents seem to be mostly fires. 

```{r}
NA_cols <- c('COMPLEX','OCC_STATUS','OCC_type','BLD_STATUS','BLD_HEIGHT','LEVEL_OF_ORIGIN','AGE_OF_STRUCTURE','SMOKE_ALARM_PRESENCE_AND_OPERATION_MAIN_FLOOR','SMOKE_ALARM_FAILURE_TO_OPERATE','SMOKE_ALARM_TYPE','SMOKE_ALARM_OTHER_FLOOR_PRESENCE','SMOKE_ALARM_ON_ALL_FLOORS','SMOKE_ALARM_IMPACT_ON_EVAC','FIRE_ALARM_SYSTEM_PRESENCE','FIRE_ALARM_SYSTEM_OPERATION','FIRE_ALARM_SYSTEM_IMPACT','SPRINKLER_SYSTEM_PRESENCE','SPRINKLER_SYSTEM_ACTIVATION')

FIRES <- I[
  !is.na(I$COMPLEX) &
  !is.na(I$OCC_STATUS) &
  !is.na(I$OCC_TYPE) &
  !is.na(I$BLD_STATUS) &
  !is.na(I$BLD_HEIGHT) & 
  !is.na(I$LEVEL_OF_ORIGIN) &
  !is.na(I$AGE_OF_STRUCTURE) &
  !is.na(I$SMOKE_ALARM_PRESENCE_AND_OPERATION_MAIN_FLOOR) &
  !is.na(I$SMOKE_ALARM_FAILURE_TO_OPERATE) &
  !is.na(I$SMOKE_ALARM_TYPE) &
  !is.na(I$SMOKE_ALARM_ON_ALL_FLOORS) &
  !is.na(I$SMOKE_ALARM_IMPACT_ON_EVAC) &
  !is.na(I$FIRE_ALARM_SYSTEM_PRESENCE) &
  !is.na(I$FIRE_ALARM_SYSTEM_IMPACT) &
  !is.na(I$SPRINKLER_SYSTEM_PRESENCE) &
  !is.na(I$SPRINKLER_SYSTEM_ACTIVATION)
,]

table(FIRES$PROPERTY_GROUP)/length(FIRES$PROPERTY_GROUP)
table(I$PROPERTY_GROUP)/length(I$PROPERTY_GROUP)


```

## Feature Selection
### INCIDENT_NUMBER & FD_STATION
As I continue my exploratory data analysis I seem to discover features that appear, at first glance, to be completely redundant. A prime example of such a feature is FD_STATION: The below call to pbapply returns 720340. Only 30 entries don't correspond to this pattern! Of that 30, 6 are NA and the other 24 are simply blank strings in the INCIDENT_NUMBER field.


```{r}
sum(substring(I$FD_STATION, 0, nchar(I$INCIDENT_NUMBER)) == I$INCIDENT_NUMBER | I$FD_STATION == '0', na.rm=T) #720364

# sum(as.numeric(gsub(I$INCIDENT_NUMBER, "", I$FD_STATION)) == 0, na.rm=T)
sum(pbapply(I[,c('INCIDENT_NUMBER','FD_STATION')],1, function(x){
  as.numeric(gsub(x['INCIDENT_NUMBER'],'',x['FD_STATION'])) == 0
}), na.rm=T) #returns 720340

in_eq_fdstn <- pbapply(I[,c('INCIDENT_NUMBER','FD_STATION')],1, function(x){
  as.numeric(gsub(x['INCIDENT_NUMBER'],'',x['FD_STATION'])) == 0
})

# As FD_STATION is a bit pointless I'm dropping it.
I$FD_STATION <- NULL
```


### Some preliminary cross-tables

```{r}
# https://www.stat.berkeley.edu/~s133/dates.html
# Some cool temporal breakdowns
table(format(I$INCIDENT_DATE, '%Y')) 
table(format(I$INCIDENT_DATE, '%A'))
table(format(I$INCIDENT_DATE, '%B'))
table(format(I$DISPATCH_DATE, '%H'))
table(I$PROPERTY)
# INCIDENT_NUMBER & FD_STATION have a lot of similarities...
sum(I$INCIDENT_NUMBER == substr(I$FD_STATION,0,9), na.rm=T) #375456
length(unique(I$FD_STATION)) #375478
```

### Grouping Property Type by Category

```{r}
# https://www.quora.com/How-do-I-get-a-frequency-count-based-on-two-columns-variables-in-an-R-dataframe?share=1

# I'm trying to build a cross-table

I$PROPERTY_GROUP <- vector(mode='character', length=nrow(I))
# I[I$PROPERTY]
PROPERTY_GROUP <- list()
PROPERTY_GROUP$A <- (101:199)
PROPERTY_GROUP$B <- (201:299)
PROPERTY_GROUP$C <- (301:399)
PROPERTY_GROUP$D <- (401:499)
PROPERTY_GROUP$E <- (501:599)
PROPERTY_GROUP$F <- (601:799)
PROPERTY_GROUP$O <- (801:999)

for(g in c(LETTERS[(1:6)],'O')){
  I$PROPERTY_GROUP[I$PROPERTY %in% PROPERTY_GROUP[[g]]] <- g
}

I[I$PROPERTY_GROUP == "", 'PROPERTY_GROUP'] <- NA
# Would you like this feature as a factor?
I$PROPERTY_GROUP <- addNA(I$PROPERTY_GROUP)
# Would you like it to be descriptively named?
levels(I$PROPERTY_GROUP) <- c('Assembly','Care & Detention','Residential','Business & Personal Services','Mercantile','Industrial','Not Classified',NA)



table(format(I$INCIDENT_DATE, '%A'), I$PROPERTY_GROUP) # This is Day of week vs Property Group
```
### Event type & group
The approach here was to combine similar-seeming event types into their own, larger group category. 
```{r}
# Firstly, EVENT_TYPE has at least 3 mistakes: 31 & 33 aren't listed as possibilities so I'm setting them to NA
I[I$ALARM_TO_FD %in% 33, "ALARM_TO_FD"] <- NA
I[I$ALARM_TO_FD %in% 31, "ALARM_TO_FD"] <- NA
# levels(I$ALARM_TO_FD) <- c(as.character(1:11), NA)
I$ALARM_TO_FD <- droplevels(I$ALARM_TO_FD)

event_typ_grp <- read.csv(file="../csv/event_type.csv", header=T)
# Below dirty one-liner doesn't seem to work... perhaps a mystery for another day.
# I$EVENT_GROUP <- merge(I, event_typ_grp, by="EVENT_TYPE")[,'EVENT_GROUP'] # Lefts inner join

# the slower, less l33t approach:
I <- merge(I, event_typ_grp, by="EVENT_TYPE")
I$Freq <- NULL
# I <- cbind(I, It[,'EVENT_GROUP'])
# I$EVENT_GROUP <- factor(It$EVENT_GROUP)

```

### Response Group
As with EVENT_TYPE and PROPERTY_TYPE, I believe grouping RESPONSE_TYPE would be useful.
```{r}
I$RESPONSE_GROUP <- vector(mode="character", length=nrow(I))

I[I$RESPONSE_TYPE %in% 1:3, 'RESPONSE_GROUP'] <- "A"
I[I$RESPONSE_TYPE %in% 11:13, 'RESPONSE_GROUP'] <- "B"
I[I$RESPONSE_TYPE %in% 21:29, 'RESPONSE_GROUP'] <- "C"
I[I$RESPONSE_TYPE %in% c(23,36), 'RESPONSE_GROUP'] <- "D"
I[I$RESPONSE_TYPE %in% c(31:35, 39), 'RESPONSE_GROUP'] <- "E"
I[I$RESPONSE_TYPE %in% 37:38, 'RESPONSE_GROUP'] <- "F"
I[I$RESPONSE_TYPE %in% c(53,41:51,54,57:59), 'RESPONSE_GROUP'] <- "G"
I[I$RESPONSE_TYPE %in% c(61:69, 601:605,698,699), 'RESPONSE_GROUP'] <- "H"
I[I$RESPONSE_TYPE %in% c(701:703,71,73:76,82,84,85,86,88,89,898,899), 'RESPONSE_GROUP'] <- "I"
I[I$RESPONSE_TYPE %in% c(921,922,910:913,92:99), 'RESPONSE_GROUP'] <- "J"

I$RESPONSE_GROUP <- factor(I$RESPONSE_GROUP, levels=LETTERS[1:10])

```



### Response Time (Time-to-arrival)

For this 'new' feature I subtracted response time, or Time-to-arrival, by subtracting the ONSCENE_TIME from the INITIAL_CALL_TIME, both of which were fairly complete in the dataset. I found that the dataset was encoded such that INCIDENT_DATE was always the date of the initial call. For cases where the call was received late at night, say 11:59pm, and the Responding Units didn't arrive until the next day I added one day (24 * 60 * 60 seconds) to the INCIDENT_DATE used for ONSCENE TIME.

I wrote a function to make this calculation, below, and passed it the rows of the dataset, one at a time, using apply. This process took some time to compute on my mid 2009 MacBook. Originally approximately 25 minutes, I was able to reduce it to 15 minutes by simplifying the function a bit and also only passing the relevant columns to apply.

```{r}
MEDICAL <- I[I$EVENT_TYPE %in% 'Medical',]
table(format(MEDICAL$INCIDENT_DATE, '%A'), MEDICAL$PROPERTY_GROUP)

# This below function calculates time difference between time call was received
# and recorded onscene time. I accounted for cases where the date would roll over between call & arrival.
# However, I later realized that the data itself is encoded to account for this: 
# DISPATCH_DATE includes the date & time of ONSCENE whereas INCIDENT_DATE is the date of call. 


tta <- function(x){
  if(sum(is.na(c(x["INCIDENT_DATE"], x["INITIAL_CALL_HOUR"], x["INITIAL_CALL_MIN"], x["INITIAL_CALL_SEC"], x["ONSCENE_HOUR"],x["ONSCENE_MIN"],x["ONSCENE_SEC"]))) > 0) return(NA)
  # onscene_date <- x["INCIDENT_DATE"]
  ic <- as.POSIXct(paste0(
    x["INCIDENT_DATE"], " ", x["INITIAL_CALL_HOUR"], ":", x["INITIAL_CALL_MIN"], ":", x["INITIAL_CALL_SEC"] 
  ), format="%Y-%m-%d %H:%M:%S", tz="EST")
  os <- as.POSIXct(paste0(
    x["INCIDENT_DATE"], " ", x["ONSCENE_HOUR"], ":", x["ONSCENE_MIN"], ":", x["ONSCENE_SEC"]
  ), format="%Y-%m-%d %H:%M:%S", tz="EST")
  if(x["ONSCENE_HOUR"] < x["INITIAL_CALL_HOUR"]){ os <- os + (24 * 60 * 60) }
  return(as.numeric(os) - as.numeric(ic))
  
}

# I[!is.na(I$INITIAL_CALL_HOUR) & !is.na(I$ONSCENE_HOUR) & I$ONSCENE_HOUR < I$INITIAL_CALL_HOUR,] #Rollover dispatch
# nrow(I[!is.na(I$INITIAL_CALL_HOUR) & !is.na(I$ONSCENE_HOUR) & I$ONSCENE_HOUR < I$INITIAL_CALL_HOUR,]) #2519

# Below is the code to actually create the 'TTA' column: First an empty numeric vector, later populated with the call to apply(). This call took about 15 minutes to run. I used pbapply, from the pabbly package, for the progress bar  (very handy!)
#
# library(pbapply)
# I$TTA <- vector(length=nrow(I), mode="numeric")
# I$TTA <- pbapply(I[,c('INCIDENT_DATE','INITIAL_CALL_HOUR','INITIAL_CALL_MIN','INITIAL_CALL_SEC','ONSCENE_HOUR','ONSCENE_MIN','ONSCENE_SEC')], 1, tta)
# 
# Since I have already calculated the TTA values I save I save time when loading the project by either saving the .RData or writing the columns as a .csv and re-loading the TTA feature back into I. 
# write.csv(x=I[,c('INCIDENT_NUMBER','TTA')], file="TTA.csv")
# TTA <- read.csv(file="../csv/TTA.csv", header = T)
# I <- cbind(I, TTA$TTA)
# names(I)[101] <- "TTA"

# Now that we have TTA we can do some cool cross-tables
tapply(I$TTA, I$PROPERTY_GROUP, mean, na.rm=T)

# Trying to test the above: Boolean "is DISPATCH_DATE" time always ONSCENE time?
sum(format(I$DISPATCH_DATE, format='%Y-%m-%d') == I$INCIDENT_DATE, na.rm=T) # 701044
sum(I$INITIAL_CALL_HOUR > I$ONSCENE_HOUR, na.rm=T) # 2519
nrow(I[!is.na(I$DISPATCH_DATE) & format(I$DISPATCH_DATE, format='%Y-%m-%d') != I$INCIDENT_DATE,]) # 2524

# There seem to be a few observations (17) that don't fit the pattern:
# Incident Date: Date initial call was received
# Dispatch Date: Date & Time first responders arrive on scene
# I'm guessing that these cases are a result of a data input error. After briefly browsing aforementioned 17 records, 
# I noticed some of the fields don't match up: Control date, when inputted, close to incident or dispach date.. Also the times used tend to be all over the place and don't correspond to other columns. 
# I might remove these observations entirely. 

nrow(I[!is.na(I$DISPATCH_DATE) & format(I$DISPATCH_DATE, format='%Y-%m-%d') != I$INCIDENT_DATE & I$INITIAL_CALL_HOUR <= I$ONSCENE_HOUR,]) # 17
# total obs - (# dispatch date = incident date) - (#edge cases) - (Dispatch Date NAs) 
# = 720370 - 701044 - 2519 - 16802  

# The below lines are to calculate edge cases. A Date is said to be "less than" another date if it occurred before the date being compared to. Example: January 1st, 1980 < January 2nd, 1980. This is easier to remember if you think of dates in terms of 'Unix seconds' wheras the integer value equality holds. 
# 
sum(I$DISPATCH_DATE > I$INCIDENT_DATE, na.rm=T) # 703568
sum(is.na(I$DISPATCH_DATE > I$INCIDENT_DATE)) # 16802
sum(I[!is.na(I$DISPATCH_DATE), 'DISPATCH_DATE'] > I[!is.na(I$DISPATCH_DATE), 'INCIDENT_DATE'])
sum(is.na(I$DISPATCH_DATE)) + sum(I[!is.na(I$DISPATCH_DATE), 'DISPATCH_DATE'] > I[!is.na(I$DISPATCH_DATE), 'INCIDENT_DATE']) == nrow(I)

```


# Some TTA Metrics. 

```{r}
aggregate(I$TTA, list(I$PROPERTY_GROUP), mean, na.rm=T)
table(I$PROPERTY_GROUP)
summary(I)
sum(I$TTA == 0, na.rm = T)
sum(I$TTA < 0, na.rm = T)

tta_eg_mdl <- lm(TTA ~ EVENT_GROUP, data = I)
summary(tta_eg_mdl)
```

### EVENT_TYPE vs ALARM_TO_FD
I am interested in the relationship between EVENT_TYPE & ALARM TO FD
```{r}
# https://stats.stackexchange.com/questions/81483/warning-in-r-chi-squared-approximation-may-be-incorrect
# chisq.test below gives warning "Chi-squared approximation may be incorrect" which is a bit
# disheartening. The above post on stackexchange advises to set simulate.p.value = T in the call to the test function. 

et_vs_atofd <- addmargins(table(I$EVENT_TYPE, I$ALARM_TO_FD), 2)
# table(I[!I$ALARM_TO_FD %in% NA, 'EVENT_GROUP'], droplevels(I[!I$ALARM_TO_FD %in% NA, 'ALARM_TO_FD']))
EVENT_GROUP <- droplevels(I[!I$ALARM_TO_FD %in% NA & !I$EVENT_GROUP %in% NA, 'EVENT_GROUP'])
ALARM_TO_FD <- droplevels(I[!I$ALARM_TO_FD %in% NA & !I$EVENT_GROUP %in% NA, 'ALARM_TO_FD'])
table(EVENT_GROUP, ALARM_TO_FD)
# I think that having so many 0s in ALARM_TO_FD 10 & 11 is giving me df = NA for chisq.test. 
# As a result I have opted to merge them and run the test again...
# Merging makes sense because the OFM Code List has both 10 & 11 as "No alarm received"

levels(ALARM_TO_FD) <- c(as.character(1:10),10)

# chisq.test(table(I$EVENT_GROUP, I$ALARM_TO_FD), simulate.p.value = T) # returns df = NA
chisq.test(table(I[!I$ALARM_TO_FD %in% NA, 'EVENT_GROUP'], droplevels(I[!I$ALARM_TO_FD %in% NA, 'ALARM_TO_FD'])), simulate.p.value = T) # Also has df = NA. I believe there were some NAs that weren't removed from EVENT_GROUP
chisq.test(table(EVENT_GROUP, ALARM_TO_FD), simulate.p.value = T)

```

### Low Variance Filter
```{r}
sum(I$FF_FATALITIES > 0) # 0
I$FF_FATALITIES <- NULL

sum(I$EST_LOSS > 0)
I$EST_LOSS_BINS <- cut(I$EST_LOSS, c(0,5000,10000,max(I$EST_LOSS)), include.lowest=T)

```

## Association Rules
```{r}
# library(arules)
# below won't run because apriori() only appreciataes factors or logical...
rules <- apriori(I[,c('EVENT_GROUP','ALARM_TO_FD','RESPONSE_GROUP','PROPERTY_GROUP','INCIDENT_MONTH','INCIDENT_DAY','INITIAL_CALL_HOUR','EVENT_TYPE_CD')], parameter = list(supp = 0.5, conf = 0.9, target="rules"))

rules <- apriori(data.frame(
  EVENT_GROUP = I$EVENT_GROUP,
  ALARM_TO_FD = I$ALARM_TO_FD,
  RESPONSE_GROUP = addNA(I$RESPONSE_GROUP),
  PROPERTY_GROUP = addNA(I$PROPERTY_GROUP),
  MONTH = I$INCIDENT_MONTH,
  DAY = I$INCIDENT_DAY,
  HOUR = addNA(I$INITIAL_CALL_HOUR),
  EVENT_TYPE_CD = I$EVENT_TYPE_CD
 ),
 parameter = list(supp = 0.1, conf = 0.1, target = "rules")
 )

rules <- apriori(data.frame(
  EVENT_GROUP = I$EVENT_GROUP,
  ALARM_TO_FD = I$ALARM_TO_FD,
  RESPONSE_GROUP = addNA(I$RESPONSE_GROUP),
  PROPERTY_GROUP = addNA(I$PROPERTY_GROUP),
  MONTH = format(I$INCIDENT_DATE, '%B'),
  DAY = I$INCIDENT_DAY,
  HOUR = addNA(I$INITIAL_CALL_HOUR),
  EVENT_TYPE_CD = I$EVENT_TYPE_CD,
  STATUS_ON_ARRIVAL = I$STATUS_ON_ARRIVAL,
  INITIAL_DETECTION = I$INITIAL_DETECTION,
  EXTENT_FIRE = I$EXTENT_FIRE,
  EXTENT_SMOKE = I$EXTENT_SMOKE,
  FIRE_CONTROL = I$FIRE_CONTROL,
  FIRE_CLASS = I$FIRE_CLASS
 ),
 parameter = list(supp = 0.1, conf = 0.1, target = "rules")
 )

# Write rules to .csv
write(rules, file ="arules_14f.csv", sep=",", quote=T, row.names=F)

rules.df <- as(rules, "data.frame")
write.csv(rules.df[order(rules.df$lift, decreasing = T), ], file="arules_14.csv")
```

### Fire Severity Level

Very Low: 0 Fatalities, 0 Injuries, Damage < \$5000
Low: 0 Fatalities, 0 Injuries, \$5000 <= Damage < \$10,000
Moderate: 0 Fatalities, 0 Injuries, \$10,000 <= Damage < \$50,000 or 0 Fatalities, 1 Injury, Damage < \$50,000
High: 0 Fatalities, Injuries <= 1, \$50,000 <= Damage < \$100,000 or Injuries > 1, Damage < \$100,000
Very High: Fatalities > 0 or Damage >= \$100,000

```{r}
I$FIRE_CLASS <- factor(x = character(length = nrow(I)), levels=c('VL','L','M','H','VH'))
I[I$CIVILIAN_FIRE_FATALITY == 0 & (I$FF_INJURIES + I$CIVILIAN_FIRE_INJURY) == 0 & I$EST_LOSS < 5000, 'FIRE_CLASS'] <- 'VL'
I[I$CIVILIAN_FIRE_FATALITY == 0 & (I$FF_INJURIES + I$CIVILIAN_FIRE_INJURY) == 0 & I$EST_LOSS >= 5000 & I$EST_LOSS < 10000, 'FIRE_CLASS'] <- 'L'
I[I$CIVILIAN_FIRE_FATALITY == 0 & (I$FF_INJURIES + I$CIVILIAN_FIRE_INJURY) == 0 & I$EST_LOSS >= 10000 & I$EST_LOSS < 50000, 'FIRE_CLASS'] <- 'M'
I[I$CIVILIAN_FIRE_FATALITY == 0 & (I$FF_INJURIES + I$CIVILIAN_FIRE_INJURY) == 1 & I$EST_LOSS < 50000, 'FIRE_CLASS'] <- 'M'
I[I$CIVILIAN_FIRE_FATALITY == 0 & (I$FF_INJURIES + I$CIVILIAN_FIRE_INJURY) <= 1 & I$EST_LOSS >= 50000 & I$EST_LOSS < 100000, 'FIRE_CLASS'] <- 'H'
I[I$CIVILIAN_FIRE_FATALITY == 0 & (I$FF_INJURIES + I$CIVILIAN_FIRE_INJURY) > 1 & I$EST_LOSS < 100000, 'FIRE_CLASS'] <- 'H'
I[I$CIVILIAN_FIRE_FATALITY > 0 | I$EST_LOSS >= 100000, 'FIRE_CLASS'] <- 'VH'

table(format(I$INCIDENT_DATE, '%Y'), addNA(I$FIRE_CLASS))
```

## Critical Incidents
The above Fire classification creates a severe (99.2/0.8) class imbalance problem. By reducing the class levels to binary and adding features we are able to investigate alternatives.
```{r}
I$CRITICAL <- factor(integer(length = nrow(I)), levels=c(0,1))
I[!I$FIRE_CLASS %in% 'VL', 'CRITICAL'] <- 1
I[I$EST_LOSS > 0, 'CRITICAL'] <- 1
I[!is.na(I$RESPONDING_UNITS) & I$RESPONDING_UNITS >= 7, 'CRITICAL'] <- 1
I[I$RESCUES > 0, 'CRITICAL'] <- 1
I[I$OFM_INVESTIGATIONS_CONTACTED %in% 1, 'CRITICAL'] <- 1
I[I$EST_VALUE_AT_RISK %in% 2:10, 'CRITICAL'] <- 1
# I[, 'CRITICAL'] <- 1
```


## Some Graphics
```{r}
# png('out.png')
barplot(table(format(I$INCIDENT_DATE, '%Y')), main="Incidents by Year", xlab="Year", ylab="Number of Incidents")
#dev.off() # saves the file. Important!

ggplot(data = I, mapping=aes(x = format(I$INCIDENT_DATE, '%Y'))) + geom_bar() + labs(x = "Year", y = 'Total Incidents', title = 'Incident Count by Year')

# Courly Coxwell plot of Event count 
ggplot(data = I, mapping=aes(x=INITIAL_CALL_HOUR)) + geom_bar() + labs(x = NULL, y = NULL, title="All Event Hours") + scale_x_continuous(breaks = 0:23) + coord_polar() + theme(plot.title = element_text(hjust = 0.5))

# Hourly Coxwell faceted by day of week 
ggplot(data = I, mapping=aes(x = INITIAL_CALL_HOUR)) + geom_bar() + facet_wrap( ~ INCIDENT_DAY, nrow=2) + coord_polar() + labs(x = NULL, y = NULL, title="Daily breakdown of event hours")

# Vertical barplot of Incidents by Event Group. Requires scales
# library(scales)
ggplot(data = I, mapping=aes(x = EVENT_GROUP)) + geom_bar() + scale_x_discrete(limits = rev(levels(I$EVENT_GROUP))) + scale_y_continuous(labels = comma) + coord_flip() + labs(x = "Incident Count", y = "Event Group", title="Incident Count by Event Group")

# Event Group portions of call source
ggplot(data = I, mapping=aes(x = EVENT_GROUP, fill=ALARM_TO_FD)) + geom_bar(position="fill") + theme_minimal()

# Incident Count by Property Group
# Note that this was created after renaming levels in I$PROPERTY_GROUP
ggplot(data = I[!I$PROPERTY_GROUP %in% NA,], mapping=aes(x = PROPERTY_GROUP)) + geom_bar() + scale_y_continuous(labels = comma)+ theme(axis.text.x = element_text(angle = 45, hjust=1)) + scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + labs(x = "Property Group", y = "Total Incidents", title="Incident Count by Property Group")

# Having Event Group on the X Axis provides a bit more information (I think?)
levels(I$ALARM_TO_FD) <- c('911','Civilian Telephone','Ambulance','Police Services','Monitoring Agency','Direct Connection','In Person','FD Radio','Other','No Alarm','FD Discovery', NA)
ggplot(I[!is.na(I$EVENT_GROUP) & !is.na(I$ALARM_TO_FD), ], mapping=aes(x = EVENT_GROUP, fill=ALARM_TO_FD)) + geom_bar(position = "fill") + scale_y_continuous(labels = comma) + theme(axis.text.x = element_text(angle = 45, hjust=1)) + scale_x_discrete(labels = function(x) str_wrap(x, width = 10))

# Black & White version of Event Group vs Alarm to FD (proportional)
# Note, renaming levels was used prior (as in above)
ggplot(I[!I$EVENT_GROUP %in% NA & !I$ALARM_TO_FD %in% NA, ], mapping = aes(x = ALARM_TO_FD, fill = EVENT_GROUP)) + geom_bar(position = "fill") + scale_y_continuous(labels = comma) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + scale_fill_grey() + labs(x = "Alarm to FD", fill = "Event Group", y = "Portion of Event Group", title = "Event Group Portion by Alarm to FD")

# Density of TTA
ggplot(data = I, mapping=aes(x = TTA)) + geom_density() + xlim(0,1000)

# This is an interesting Coxwell breakdown of incidents by month, faceted by year
ggplot(data = I, mapping = aes(x = MONTH) ) + geom_bar() + facet_wrap( ~ YEAR) + coord_polar()


```

## Initial Model
My plan is to work with fires only for the time being.
Train/CV/Test
```{r}
set.seed(42)
# FIRE.SET <- list()
idx <- list()
idx$fire.idx <- which(I$EVENT_GROUP %in% 'Fire')
idx$train <- sample(idx$fire.idx, 0.6*length(idx$fire.idx))
idx$cv <- sample(idx$fire.idx[!idx$fire.idx %in% idx$train], length(idx$fire.idx[!idx$fire.idx %in% idx$train]) * 0.5)
idx$test <- idx$fire.idx[!idx$fire.idx %in% idx$train & !idx$fire.idx %in% idx$cv]

# c.model <- glm(CRITICAL ~ ., family = binomial(link='logit'), data=I[idx$train,])  
# Below Model uses: EVENT_TYPE_CD, ALARM_TO_FD, AID_TO_FROM_OTHER_DEPTS, INITIAL CALL HOUR, MIN, SEC, RESPONDING UNITS
c.model <- glm(CRITICAL ~ ., family = binomial(link='logit'), data=I[idx$train, c(4, 24, 10, 23, 21, 12:14, 8, 105)])
# anova(c.model)

y <- data.frame(prob=vector(mode='numeric',length=length(idx$cv)))
y$prob <- predict(c.model, newdata=I[idx$cv, c(4,24,10,23,21,12:14,8)])
y$pred <- ifelse(y$prob > 0.5,1,0)
y$CRITICAL <- I[idx$cv, 'CRITICAL']
sum(y$pred == y$CRITICAL)/nrow(y) # Accuracy: 89%
sum(y$CRITICAL == 0)/nrow(y) # Had we simply predicted 0 for everything: 79%
(idx$cf <- as.matrix(table(y$pred, y$CRITICAL))) #confusion matrix
(custom.summaries(idx$cf))
# Accuray: 0.895
# FNR: 0.489
# PRECISION: 0.973
# RECALL: 0.510
# F1Score: 0.670


d.model <- glm(CRITICAL ~ EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + AID_TO_FROM_OTHER_DEPTS + EST_KM + INITIAL_UNIT_PERSONNEL + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR, family=binomial(link='logit'), data = I[idx$train,])
y <- data.frame(prob=vector(mode='numeric',length=length(idx$cv)))
y$prob <- predict(d.model, newdata=I[idx$cv, c('EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','AID_TO_FROM_OTHER_DEPTS','EST_KM','INITIAL_UNIT_PERSONNEL','INCIDENT_DAY','MONTH','YEAR')])
y$pred <- ifelse(y$prob > 0.5,1,0)
y$CRITICAL <- I[idx$cv, 'CRITICAL']
sum(y$pred == y$CRITICAL)/nrow(y) # Accuracy: 89%
sum(y$CRITICAL == 0)/nrow(y) # Had we simply predicted 0 for everything: 79%
(idx$cf <- as.matrix(table(y$pred, y$CRITICAL))) #confusion matrix
idx$cf[1,2]/(idx$cf[1,2] + idx$cf[2,2]) # FN Rate: 0.3112206
idx$cf[2,2]/(idx$cf[2,2] + idx$cf[2,1]) # Precision: 0.9057827
idx$cf[2,2]/(idx$cf[2,2] + idx$cf[1,2]) # Recall: 0.6887794
(2 * idx$cf[2,2]) / ((2 * idx$cf[2,2]) + idx$cf[2,1] + idx$cf[1,2]) # F1-Score: 0.7825151




# I was curious as to whether or not EVENT_TYPE would make a difference: Nope. 
e.model <- glm(CRITICAL ~ EVENT_TYPE + EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + AID_TO_FROM_OTHER_DEPTS + EST_KM + INITIAL_UNIT_PERSONNEL + INCIDENT_DAY + MONTH + YEAR, family=binomial(link='logit'), data = I[idx$train,])

e.model.cm <- table(predict.logistic(d.model, I[idx$cv, c('EVENT_TYPE','EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','AID_TO_FROM_OTHER_DEPTS','EST_KM','INITIAL_UNIT_PERSONNEL','INCIDENT_DAY','MONTH','YEAR')])$pred, I[idx$cv, 'CRITICAL'])

table(predict.logistic(d.model, I[idx$cv, c('EVENT_TYPE','EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','AID_TO_FROM_OTHER_DEPTS','EST_KM','INITIAL_UNIT_PERSONNEL','INCIDENT_DAY','INCIDENT_MONTH','INCIDENT_YEAR')])$pred, I[idx$cv, 'CRITICAL'])

# I am having a somewhat ambitious moment wondering if I can train logistic regression
# on the entire dataset. First I should probably remove NAs...

nrow(na.omit(I[,c('EVENT_TYPE','EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','AID_TO_FROM_OTHER_DEPTS','EST_KM','INITIAL_UNIT_PERSONNEL','INCIDENT_DAY','MONTH','YEAR')])) # 0

full_idx <- list()
full_idx$train <- sample(1:nrow(I), 0.6*nrow(I))
full_idx$cv <- sample((1:nrow(I))[-(full_idx$train)], 0.2*nrow(I))
full_idx$test <- (1:nrow(I))[-c(full_idx$train, full_idx$cv)]

(length(full_idx$train) + length(full_idx$test) + length(full_idx$cv)) == nrow(I) # TRUE

f.model <- glm(CRITICAL ~ EVENT_TYPE + EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + AID_TO_FROM_OTHER_DEPTS + EST_KM + INITIAL_UNIT_PERSONNEL + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR, family=binomial(link='logit'), data = I[full_idx$train,])

full.logistic.cm <- table(predict.logistic(f.model, I[full_idx$cv, c('EVENT_TYPE', 'EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','AID_TO_FROM_OTHER_DEPTS','EST_KM','INITIAL_UNIT_PERSONNEL','INCIDENT_DAY','INCIDENT_MONTH','INCIDENT_YEAR')])$pred, I[full_idx$cv, 'CRITICAL'])

## There's an issue with my above sampling method:
## The training set didn't have all factor levels
## and so you can't then use the model for predictions
## if they exist in the test set. 

table(I[full_idx$train, 'EVENT_TYPE'])[table(I[full_idx$train, 'EVENT_TYPE']) ==0]
sum(I$EVENT_TYPE %in% 'Mutual Aid Assist - Medical') # 1
sum(I$EVENT_TYPE %in% 'Police Assist - Bomb Standby - Non Emergency') # 1
sum(I$EVENT_TYPE %in% 'Rescue - Elevator - Non Emergency') # 2

# I need to add the above indexes to the trianing set so that they are available to be used in train or cv

full_idx$train <- c(full_idx$train, which(I$EVENT_TYPE %in% 'Mutual Aid Assist - Medical'))
full_idx$cv <- full_idx$cv[full_idx$cv != which(I$EVENT_TYPE %in% 'Mutual Aid Assist - Medical')]
full_idx$train <- c(full_idx$train, which(I$EVENT_TYPE %in% 'Police Assist - Bomb Standby - Non Emergency'))
full_idx$test <- full_idx$test[full_idx$test != which(I$EVENT_TYPE %in% 'Police Assist - Bomb Standby - Non Emergency')]
full_idx$train <- c(full_idx$train, which(I$EVENT_TYPE %in% 'Rescue - Elevator - Non Emergency')[1])
full_idx$cv <- full_idx$cv[full_idx$cv != which(I$EVENT_TYPE %in% 'Rescue - Elevator - Non Emergency')[1]]
full_idx$train <- c(full_idx$train, which(I$EVENT_TYPE %in% 'Police Assist - Joint Operations - Non Emergency'))
full_idx$cv <- full_idx$cv[full_idx$cv != which(I$EVENT_TYPE %in% 'Police Assist - Joint Operations - Non Emergency')]
full_idx$train <- c(full_idx$train, which(I$EVENT_TYPE %in% 'Vehicle Fire - Natural Gas'))
full_idx$cv <- full_idx$cv[full_idx$cv != which(I$EVENT_TYPE %in% 'Vehicle Fire - Natural Gas')]

(length(full_idx$train) + length(full_idx$test) + length(full_idx$cv)) == nrow(I) # TRUE
length(duplicated(c(full_idx$train, full_idx$test, full_idx$cv))) == length(c(full_idx$train, full_idx$test, full_idx$cv))

table(I[full_idx$train, 'EVENT_TYPE_CD'])[table(I[full_idx$train, 'EVENT_TYPE_CD']) ==0]

full_idx$train <- c(full_idx$train, which(I$EVENT_TYPE_CD %in% 'MEPACP'))
full_idx$cv <- full_idx$cv[full_idx$cv != which(I$EVENT_TYPE_CD %in% 'MEPACP')]


table(I[full_idx$train, 'INCIDENT_YEAR'])[table(I[full_idx$train, 'INCIDENT_YEAR']) ==0]


```

### Logistic Regression with Interaction

```{r}
# Trying some interaction terms.
lr.i.model_a <- glm(CRITICAL ~ EVENT_TYPE + EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + AID_TO_FROM_OTHER_DEPTS + EST_KM + INITIAL_UNIT_PERSONNEL + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR + EVENT_TYPE:ALARM_TO_FD, family=binomial(link='logit'), data = I[idx$train,])

table(predict.logistic(lr.i.model_a, I[idx$cv, c('EVENT_TYPE','EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','AID_TO_FROM_OTHER_DEPTS','EST_KM','INITIAL_UNIT_PERSONNEL','INCIDENT_DAY','INCIDENT_MONTH','INCIDENT_YEAR')])$pred, I[idx$cv, 'CRITICAL'])

lr.i.model_b <- glm(CRITICAL ~ EVENT_TYPE + EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + AID_TO_FROM_OTHER_DEPTS + EST_KM + INITIAL_UNIT_PERSONNEL + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR + EVENT_TYPE:ALARM_TO_FD + EVENT_TYPE:RESPONSE_TYPE, family=binomial(link='logit'), data = I[idx$train,])

full_lr.i.model_c <- glm(CRITICAL ~ EVENT_GROUP + EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_GROUP + AID_TO_FROM_OTHER_DEPTS + EST_KM + INITIAL_UNIT_PERSONNEL + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR + EVENT_GROUP:ALARM_TO_FD + EVENT_GROUP:RESPONSE_GROUP, family=binomial(link='logit'), data = I[full_idx$train,])

full.logistic.i_c.cm <- table(predict.logistic(full_lr.i.model_c, I[full_idx$cv, c('EVENT_GROUP', 'EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_GROUP','AID_TO_FROM_OTHER_DEPTS','EST_KM','INITIAL_UNIT_PERSONNEL','INCIDENT_DAY','INCIDENT_MONTH','INCIDENT_YEAR')])$pred, I[full_idx$cv[-which(full_idx$cv %in% which(I$EVENT_TYPE_CD %in% c('ISPCF',NA)))], 'CRITICAL'])

I[full_idx$cv[-which(full_idx$cv %in% which(I$EVENT_TYPE_CD %in% c('ISPCF',NA)))], 'EVENT_TYPE_CD']


df.ROC <- rbind(data.frame(predictor = predict(d.model, I[idx$cv,]), known.truth = I[idx$cv, 'CRITICAL'], model=1), data.frame(predictor = predict(lr.i.model_a, I[idx$cv,]), known.truth = I[idx$cv, 'CRITICAL'], model=2), data.frame(predictor = predict(lr.i.model_b, I[idx$cv,]), known.truth = I[idx$cv, 'CRITICAL'], model=3))

```

## LR With Interaction II
When I originally did the LR with Interaction I was using AID_TO_FROM_OTHER_DEPTS which I later noticed is filled out per the OFM Standard Incidents Code list. There's 0 NA values so I think this feature is inputted by the CAD system. However, since it's odd, I'm going to try a model or two without it.

```{r}
lr.i.model_a2 <- glm(CRITICAL ~ EVENT_TYPE + EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + EST_KM + INITIAL_UNIT_PERSONNEL + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR + EVENT_TYPE:ALARM_TO_FD, family=binomial(link='logit'), data = I[idx$train,])

table(predict.logistic(lr.i.model_a, I[idx$cv, c('EVENT_TYPE','EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','EST_KM','INITIAL_UNIT_PERSONNEL','INCIDENT_DAY','INCIDENT_MONTH','INCIDENT_YEAR')])$pred, I[idx$cv, 'CRITICAL'])

lr.i.model_b2 <- glm(CRITICAL ~ EVENT_TYPE + EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + EST_KM + INITIAL_UNIT_PERSONNEL + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR + EVENT_TYPE:ALARM_TO_FD + EVENT_TYPE:RESPONSE_TYPE, family=binomial(link='logit'), data = I[idx$train,])
```


## Summaries
I keep copy/pasting the above FN, Precision, Recall summaries and it's driving me bonkers.

```{r}
custom.summaries <- function(cm){
  # This will only work (properly) with a 2 way confusion matrix
  cm <- as.matrix(cm)
  if(mean(c(2,2) == dim(cm)) != 1) stop("This function presently only works with 2x2 Confusion Matrix (sorry!)")
  summaries <- list()
  summaries$ACCURACY <- (cm[1,1] + cm[2,2]) / sum(cm) # Accuracy
  summaries$FNR <- cm[1,2]/(cm[1,2] + cm[2,2]) # FN Rate
  summaries$PRECISION <- cm[2,2]/(cm[2,2] + cm[2,1]) # Precision
  summaries$RECALL <- cm[2,2]/(cm[2,2] + cm[1,2]) # Recall
  summaries$F1Score <- (2 * cm[2,2]) / ((2 * cm[2,2]) + cm[2,1] + cm[1,2]) # F1-Score
  return(summaries)
}

predict.logistic <- function(model, test, prob=0.5){
  y <- data.frame(prob=vector(mode='numeric',length=nrow(test)))
  y$prob <- predict(model, newdata=test)
  y$pred <- ifelse(y$prob > prob,1,0)
  # y$CRITICAL <- I[idx$cv, 'CRITICAL']
  return(y)
}
```

## K-fold cross validation
I've run into a sticky problem. In order to do K-Fold cross validation (as opposed to Hold One Out) the training set needs to have at least one observation from every level of factor. 

```{r}
FireSet <- I[I$EVENT_GROUP %in% 'Fire', ]
source("refold.models.R")
metric.lr <- refold.models(FireSet)

source("refold.nb.R")
metric.nb <- refold.nb(FireSet)

source("refold.rf.R")
metric.rf <- refold.rf(FireSet)

```



### Naive Bayes
```{r}
# install.packages("e1071")
# library(e1071)
I$EST_KM.f <- as.factor(I$EST_KM)
I$INITIAL_UNIT_PERSONNEL.f <- as.factor(I$INITIAL_UNIT_PERSONNEL)
# I$YEAR <- as.factor(I$YEAR)
# I$MONTH <- as.factor(I$MONTH)

model.nb <- naiveBayes(CRITICAL ~ EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + AID_TO_FROM_OTHER_DEPTS + EST_KM.f + INITIAL_UNIT_PERSONNEL.f + INCIDENT_DAY + MONTH + YEAR, data = I[idx$train, ])

nb <- list()

nb$cm <- table(predict(
  model.nb,
  I[idx$cv, c('EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','AID_TO_FROM_OTHER_DEPTS','EST_KM.f','INITIAL_UNIT_PERSONNEL.f','INCIDENT_DAY','MONTH','YEAR')]), I[idx$cv, 'CRITICAL'])
# nb$cf <- as.matrix(nb$cf)

(custom.summaries(nb$cm))
# ACCURACY: 0.912
# FNR: 0.289
# PRECISION: 0.843
# RECALL: 0.71
# F1-SCORE: 0.771

## Full Naive Bayes?
f.model.nb <- naiveBayes(CRITICAL ~ EVENT_TYPE_CD + ALARM_TO_FD + RESPONSE_TYPE + AID_TO_FROM_OTHER_DEPTS + EST_KM.f + INITIAL_UNIT_PERSONNEL.f + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR, data = I[full_idx$train, ])

nb$cm <- table(predict(
  f.model.nb,
  I[full_idx$cv, c('EVENT_TYPE_CD','ALARM_TO_FD','RESPONSE_TYPE','AID_TO_FROM_OTHER_DEPTS','EST_KM.f','INITIAL_UNIT_PERSONNEL.f','INCIDENT_DAY','INCIDENT_MONTH','INCIDENT_YEAR')]), I[full_idx$cv, 'CRITICAL'])
```

## Random Forest

```{r}
# install.packages("randomForest")
# library(randomForest)

## There's a max of 53 levels per category for randomForest(). I've opted to drop levels without values in EVENT_TYPE_CD and
## use RESPONSE_GROUP

# ForestSet.train <- I[idx$train,]
# ForestSet.train$EVENT_TYPE_CD <- droplevels(ForestSet.train$EVENT_TYPE_CD)

I$EVENT_TYPE_CD.c <- as.character(I$EVENT_TYPE_CD)
I$EVENT_TYPE_CD.dl <- factor(I$EVENT_TYPE_CD, levels=levels(droplevels(I[I$EVENT_GROUP %in% 'Fire', 'EVENT_TYPE_CD'])))

d.model.rf <- randomForest(
  CRITICAL ~ EVENT_TYPE_CD.dl + ALARM_TO_FD + RESPONSE_GROUP + AID_TO_FROM_OTHER_DEPTS + EST_KM.f + INITIAL_UNIT_PERSONNEL.f + INCIDENT_DAY + INCIDENT_MONTH + INCIDENT_YEAR, 
  I[idx$train,]
  )

I$ALARM_TO_FD.o <- I$ALARM_TO_FD
# I$ALARM_TO_FD <- factor(I$ALARM_TO_FD, levels=levels(I$ALARM_TO_FD)[-length(levels(I$ALARM_TO_FD))])
I$ALARM_TO_FD <- factor(I$ALARM_TO_FD.o)
I$AID_TO_FROM_OTHER_DEPTS.o <- I$AID_TO_FROM_OTHER_DEPTS 
# I$AID_TO_FROM_OTHER_DEPTS <- factor(I$AID_TO_FROM_OTHER_DEPTS, levels=levels(I$AID_TO_FROM_OTHER_DEPTS)[-length(levels(I$AID_TO_FROM_OTHER_DEPTS))])
I$AID_TO_FROM_OTHER_DEPTS <- factor(I$AID_TO_FROM_OTHER_DEPTS.o)
# I$EST_KM.f <- factor(I$EST_KM)
I$EST_KM.f <- cut(I$EST_KM, breaks=seq(0,100, by=2))
I$INITIAL_UNIT_PERSONNEL.f <- factor(I$INITIAL_UNIT_PERSONNEL)

y.rf <- predict(d.model.rf, I[idx$cv,c('EVENT_TYPE_CD.dl','ALARM_TO_FD','RESPONSE_GROUP','AID_TO_FROM_OTHER_DEPTS','EST_KM.f','INITIAL_UNIT_PERSONNEL.f','INCIDENT_DAY','INCIDENT_MONTH','INCIDENT_YEAR')])
table(y.rf, I[idx$cv, 'CRITICAL'])
custom.summaries(table(y.rf, I[idx$cv, 'CRITICAL']))
```

## Comparing the Models
I think it's time to combine all of my output metrics from the 10x model runs and perhaps make some pretty charts

```{r}
metrics.all <- rbind(metrics.lr, metric.nb$metrics, metric.rf$metrics)
metrics.all$model <- factor(rep(c('LR','NB','RF'), each=10))

metrics.stack <- stack(metrics.all[,1:4])
metrics.stack$model <- factor(rep(c('LR','NB','RF'), each=10))
metrics.stack$run <- 1:10
ggplot(data = metrics.stack[!metrics.stack$ind %in% 'FNR',], mapping=aes(x=run, y=values, colour=model, linetype=ind, points=ind)) + geom_point() + geom_line() + ylim(0.6,1)

# Below has the hline NULL model
ggplot(data = metrics.stack[!metrics.stack$ind %in% 'FNR',], mapping=aes(x=run, y=values, colour=model, linetype=ind, points=ind)) + geom_point() + geom_line() + ylim(0.6,1) + scale_x_continuous(breaks=1:10) + labs(x='Iteration', y = 'Percent', title='Accuracy, Precision & Recall for 3 Models') + geom_hline(yintercept=0.79) + annotate("text",4, 0.79, label="ACCURACY {CRITICAL = 0}", vjust=-0.5)

## Above I forgot F1-Score! oops
metrics.stack2 <- stack(metrics.all[,c(1,3:5)])
metrics.stack2$model <- factor(rep(c('LR','NB','RF'), each=10))
metrics.stack2$run <- 1:10

ggplot(data = metrics.stack2, mapping=aes(x=run, y=values, colour=model, shape=ind, linetype=ind)) + geom_point() + geom_line() + ylim(0.65,0.95) + scale_x_continuous(breaks=1:10) + labs(x='Iteration', y = 'Percent', title='Accuracy, Precision, Recall\n& F1-Score for 3 Models')

metrics.stack3 <- stack(metrics.all2[,c(1,3:5)])
metrics.stack3$model <- factor(rep(c('LR','NB','RF'), each=10))
metrics.stack3$run <- 1:10

ggplot(data = metrics.stack3, mapping=aes(x=run, y=values, colour=model, shape=ind, linetype=ind)) + geom_point() + geom_line() + ylim(0.65,0.95) + scale_x_continuous(breaks=1:10) + labs(x='Iteration', y = 'Percent', title='Accuracy, Precision, Recall\n& F1-Score for 3 Models') + scale_y_continuous(labels = scales::percent)
```

## One way ANOVA

```{r}
fit <- lm(values ~ model, data = metrics.stack3[metrics.stack3$ind %in% 'F1Score',])
anova(fit)
fit <- lm(values ~ model, data = metrics.stack3[metrics.stack3$ind %in% 'ACCURACY',])
anova(fit)

```

